{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e3AaPphVQtfQ"
      },
      "source": [
        "# Case Study 3 : Textual analysis of movie reviews"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v9xFcGCJQtfS"
      },
      "source": [
        "**Due Date: December 5, 2023, BEFORE the beginning of class at 12:00pm ET**\n",
        "\n",
        "NOTE: There are always last minute issues submitting the case studies. DO NOT WAIT UNTIL THE LAST MINUTE!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W96Ki3enQtfT"
      },
      "source": [
        "<img src=\"https://miro.medium.com/v2/resize:fit:720/format:webp/1*o-qaS9WPD9ocA9Ofr85v5g.png\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0z6U9nIKQtfT"
      },
      "source": [
        "**TEAM Members:** Please EDIT this cell and add the names of all the team members in your team\n",
        "\n",
        "    member 1\n",
        "    \n",
        "    member 2\n",
        "    \n",
        "    ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SeewMOzEQtfT"
      },
      "source": [
        "**Desired outcome of the case study.**\n",
        "* In this case study we will look at movie reviews from the v2.0 polarity dataset comes from\n",
        "the http://www.cs.cornell.edu/people/pabo/movie-review-data.\n",
        "    * It contains written reviews of movies divided into positive and negative reviews.\n",
        "* As in Case Study 2 idea is to *analyze* the data set, make *conjectures*, support or refute those conjectures with *data*, and *tell a story* about the data!\n",
        "    \n",
        "**Required Readings:**\n",
        "* This case study will be based upon the scikit-learn Python library\n",
        "* We will build upon the tutorial \"Working With Text Data\" which can be found at http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html\n",
        "* In particular, this case study is quite similar to \"Exercise 2: Sentiment Analysis on movie reviews\" on the above web page.\n",
        "* Read about deep learning at https://scikit-learn.org/stable/modules/neural_networks_supervised.html\n",
        "\n",
        "\n",
        "**Case study assumptions:**\n",
        "* You have access to a python installation\n",
        "\n",
        "**Required Python libraries:**\n",
        "* Numpy (www.numpy.org) (should already be installed from Case Study 2)\n",
        "* Matplotlib (matplotlib.org) (should already be installed from Case Study 2)\n",
        "* Scikit-learn (scikit-learn.org).\n",
        "* You are also welcome to use the Python Natural Language Processing Toolkit (www.nltk.org) (though it is not required).\n",
        "\n",
        "** NOTE **\n",
        "* Please don't forget to save the notebook frequently when working in IPython Notebook, otherwise the changes you made can be lost."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQQrXatF30aY"
      },
      "source": [
        "# Getting the data onto Colab example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ludq2Spg3zdC"
      },
      "outputs": [],
      "source": [
        "! wget https://www.cs.cornell.edu/people/pabo/movie-review-data/review_polarity.tar.gz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j6lyvK0T4HCk"
      },
      "source": [
        "Look for the directory txt_sentoken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QqAfVpEJ4A0P"
      },
      "outputs": [],
      "source": [
        "! tar xzf review_polarity.tar.gz\n",
        "! ls"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7m-jzLoyQtfU"
      },
      "source": [
        "## Problem 1 (10 points): Complete Exercise 2: Sentiment Analysis on movie reviews from http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QyZeq4COQtfU"
      },
      "source": [
        "* Installing scikit-learn using Anaconda does not necessarily download the example source-code.\n",
        "* Accordingly, you may need to download these directly from Github at https://github.com/scikit-learn/scikit-learn:\n",
        "    * The data can be downloaded using doc/tutorial/text_analytics/data/movie_reviews/fetch_data.py\n",
        "    * A skeleton for the solution can be found in doc/tutorial/text_analytics/skeletons/exercise_02_sentiment.py\n",
        "    * A completed solution can be found in doc/tutorial/text_analytics/solutions/exercise_02_sentiment.py\n",
        "* Here is a direct link to the code to help you out:  https://github.com/scikit-learn/scikit-learn/tree/main/doc/tutorial/text_analytics\n",
        "* **It is ok to use the solution provided in the scikit-learn distribution as a starting place for your work.**\n",
        "\n",
        "### Modify the solution to Exercise 2 so that it can run in this iPython notebook\n",
        "* This will likely involve moving around data files and/or small modifications to the script."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x9rv82MAQtfV"
      },
      "outputs": [],
      "source": [
        "#----------------------------------------------\n",
        "# Your code starts here\n",
        "#   Please add comments or text cells in between to explain the general idea of each block of the code.\n",
        "#   Please feel free to add more cells below this cell if necessary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-1cVyJKHQtfV"
      },
      "source": [
        "## Problem 2 (10 points): Explore the scikit-learn TfidVectorizer class\n",
        "\n",
        "**Read the documentation for the TfidVectorizer class at http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html.**\n",
        "* Define the term frequencyâ€“inverse document frequency (TF-IDF) statistic (http://en.wikipedia.org/wiki/Tf%E2%80%93idf will likely help).\n",
        "* Run the TfidVectorizer class on the training data above (docs_train).\n",
        "* Explore the min_df and max_df parameters of TfidVectorizer.  What do they mean? How do they change the features you get?\n",
        "* Explore the ngram_range parameter of TfidVectorizer.  What does it mean? How does it change the features you get? (Note, large values  of ngram_range may take a long time to run!)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "YGCOW1yjQtfW"
      },
      "outputs": [],
      "source": [
        "#----------------------------------------------\n",
        "# Your code starts here\n",
        "#   Please add comments or text cells in between to explain the general idea of each block of the code.\n",
        "#   Please feel free to add more cells below this cell if necessary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jVWQAJmsQtfW"
      },
      "source": [
        "## Problem 3 (15 points): Machine learning algorithms\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PNt6Ue6-QtfW"
      },
      "source": [
        "* Based upon Problem 2 pick some parameters for TfidfVectorizer\n",
        "    * \"fit\" your TfidfVectorizer using docs_train\n",
        "    * Compute \"Xtrain\", a Tf-idf-weighted document-term matrix using the transform function on docs_train\n",
        "    * Compute \"Xtest\", a Tf-idf-weighted document-term matrix using the transform function on docs_test\n",
        "    * Note, be sure to use the same Tf-idf-weighted class (**\"fit\" using docs_train**) to transform **both** docs_test and docs_train\n",
        "* Examine two classifiers provided by scikit-learn\n",
        "    * LinearSVC\n",
        "    * KNeighborsClassifier\n",
        "    * Why do you think it might be working better?\n",
        "* For a particular choice of parameters and classifier, look at 2 examples where the prediction was incorrect.\n",
        "    * Can you conjecture on why the classifier made a mistake for this prediction?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "m945iZIxQtfW"
      },
      "outputs": [],
      "source": [
        "#----------------------------------------------\n",
        "# Your code starts here\n",
        "#   Please add comments or text cells in between to explain the general idea of each block of the code.\n",
        "#   Please feel free to add more cells below this cell if necessary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BoicR0YeQx5x"
      },
      "source": [
        "## Problem 4 (15 points): Using pre-trained models trained from Hugging Face\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Installing some necessary packages for this problem"
      ],
      "metadata": {
        "id": "Ss9kstM7vOq9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
        "!pip install transformers[torch]\n",
        "!pip install evaluate"
      ],
      "metadata": {
        "id": "q_IxebKFg4U6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Checking That The Notebook Has a GPU.**\n",
        "\n",
        "#### **This is very important, as the fine-tuning will take very long, or the notebook may even crash, without having a GPU. Do not continue with the rest of problem 4 without seeing \"Sucess!\" in the printout from the cell below.**"
      ],
      "metadata": {
        "id": "Bh9Ns33pvUG4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "      device = torch.device(\"cuda:0\")\n",
        "      print(f\"Success!\\nUsing GPU: ({torch.cuda.get_device_name(device=device)})\")\n",
        "else:\n",
        "      print(\"\\nNo GPU found.\\n\\nDO NOT CONTINUE TO PROBLEM 4 or 5 UNTIL YOU SEE \\\"Success!\\\" PRINTED OUT.\\n\\nDo the following:\\n1. Save your colab file.\\n2. Click on Runtime -> Change runtime type -> T4 GPU -> OK (Don't worry about losing progress if the runtime needs to restart, we just saved the file in step #1) -> Save.\\n3. Wait 5-10 sec.\\n5. Click on Runtime -> View Resources. You should see \\\"GPU RAM\\\" as one of the charts.\\n5. Rerun this cell.\")\n",
        "      device = torch.device(\"cpu\")\n"
      ],
      "metadata": {
        "id": "EIooYQflfZ_y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Using Hugging Face Models \"Off-The-Shelf\"\n",
        "\n",
        "Go to [hugging face's model hub](https://huggingface.co/models) and search for a sentiment model.\n",
        "\n",
        "Can you find one for reviews, movies, or something else that fits the problem well?\n",
        "\n",
        "Only use models that have the following:\n",
        "- Have summary statistics of its performance on a test or validation dataset  \n",
        "- Have a python API (you should see a window for it at the bottom of the page)\n",
        "- Take **RAW TEXT** (not the vectorized words) as input into the model\n",
        "- Output **POSITIVE** or **NEGATIVE** (if its output is pos/neg/neutral, this is fine, but you will need to transform this output to only pos/neg and described how you handle **neutral** outputs)\n",
        "\n",
        "Report the url of the model's page you found in the box below.\n",
        "\n",
        "In the code cell below, evaluate the performance of your chosen pre-trained model on the v2.0 polarity dataset. What's the accuracy, F1 Score, and other metrics.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_WgMZvm8d1a3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####URL TO MODEL PAGE: _________________\n",
        "####Hugging Face Reported TEST/VALIDATION PERFORMANCE: _________________\n",
        "####v2.0 Polarity Dataset TEST PERFORMANCE: _________________"
      ],
      "metadata": {
        "id": "tJGyHJjLm8uS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer, TrainingArguments, Trainer"
      ],
      "metadata": {
        "id": "_aZ-nXthp-C4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#----------------------------------------------\n",
        "# Your code starts here\n",
        "#   Please add comments or text cells in between to explain the general idea of each block of the code.\n",
        "#   Please feel free to add more cells below this cell if necessary\n"
      ],
      "metadata": {
        "id": "Yns9uS__fZZI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Using A PyTorch Model From Hugging Face\n",
        "\n",
        "Here, we will be using a movie sentiment model made by JamesH. The model card can be found [here](JamesH/Movie_review_sentiment_analysis_model).\n",
        "\n",
        "Report the url of the model's page you found in the box below.\n",
        "\n",
        "In the code cells below, evaluate the performance of your chosen pre-trained model on the v2.0 polarity dataset. What's the accuracy, F1 Score, and other metrics?"
      ],
      "metadata": {
        "id": "Dkd-VUIes7_F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####URL TO MODEL PAGE: _________________\n",
        "####Hugging Face Reported TEST/VALIDATION PERFORMANCE: _________________\n",
        "####v2.0 Polarity Dataset TEST PERFORMANCE: _________________"
      ],
      "metadata": {
        "id": "-cl7ROCrthjU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Loading the model"
      ],
      "metadata": {
        "id": "ArE6GS1NtyKi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LaDYg8XNQx5y"
      },
      "outputs": [],
      "source": [
        "james_h_model = AutoModelForSequenceClassification.from_pretrained(\"JamesH/autotrain-third-project-1883864250\")\n",
        "james_h_tokenizer = AutoTokenizer.from_pretrained(\"JamesH/autotrain-third-project-1883864250\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Example API of how to interface with the JamesH model"
      ],
      "metadata": {
        "id": "ttsQ177-tsnI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_phrase = \"That movie wasn't good. For several reasons. Firstly, there isn't anything special about it\"\n",
        "with torch.no_grad():\n",
        "  inputs = james_h_tokenizer(input_phrase, return_tensors=\"pt\")\n",
        "  outputs = james_h_model(**inputs)\n",
        "  print(outputs)\n",
        "  # Output is a tuple containing the logits of the positive and negative predictions\n",
        "  # We need to convert these to probabilities\n",
        "  logits = outputs[\"logits\"]\n",
        "  odds = torch.exp(logits)\n",
        "  probabilities = odds / (1 + odds)\n",
        "  print(probabilities)\n",
        "  # Lets make it look more readable\n",
        "  positive = probabilities[0,0]\n",
        "  negative = probabilities[0,1]\n",
        "  print(\"\\n\")\n",
        "  print(f\"Phrase: {input_phrase}\\nPositive: {positive:.3f}\\nNegative: {negative:.3f}\")\n",
        "\n",
        "  if positive > 0.5:\n",
        "    print(f\"The phrase has positive sentiment\")\n",
        "  else:\n",
        "    print(f\"The phrase has negative sentiment\")"
      ],
      "metadata": {
        "id": "Jgy91zHuLOPe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#----------------------------------------------\n",
        "# Your code starts here\n",
        "#   Please add comments or text cells in between to explain the general idea of each block of the code.\n",
        "#   Please feel free to add more cells below this cell if necessary"
      ],
      "metadata": {
        "id": "FT7YoS8jt6OC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Fine-tuning a Pre-Trained Model On Our Dataset\n",
        "\n",
        "Here, we will be using the JamesH model as the starting point of training a new model that's trained specifically for our dataset\n",
        "\n",
        "You will be tasked with finding hyperparameters to use during the fine-tuning process that improves the original model's performance.\n",
        "\n",
        "Then you will report the new model performance on the fine-tuned model."
      ],
      "metadata": {
        "id": "ZrfQ8mbKeJlM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import random\n",
        "import datasets\n",
        "import numpy as np\n",
        "import evaluate\n",
        "\n",
        "df_train = pd.DataFrame(columns=['label', 'text'])\n",
        "df_test = pd.DataFrame(columns=['label', 'text'])\n",
        "\n",
        "pos_reviews = []\n",
        "neg_reviews = []\n",
        "\n",
        "# Getting all positive reviews from disk\n",
        "for filename in os.listdir(\"./txt_sentoken/pos\"):\n",
        "    with open(f\"./txt_sentoken/pos/{filename}\", \"r\") as f:\n",
        "        pos_reviews.append(f.read())\n",
        "\n",
        "# Getting all negative reviews from disk\n",
        "for filename in os.listdir(\"./txt_sentoken/neg\"):\n",
        "    with open(f\"./txt_sentoken/neg/{filename}\", \"r\") as f:\n",
        "        neg_reviews.append(f.read())\n",
        "\n",
        "# Randomly shuffle both lists for splitting into training and test sets\n",
        "random.shuffle(pos_reviews)\n",
        "random.shuffle(neg_reviews)\n",
        "\n",
        "# Add each review into the dataset variable\n",
        "# This dataset format is compatible for most Hugging Face wrappers in PyTorch\n",
        "test_percentage = 0.2\n",
        "\n",
        "for i in range(len(neg_reviews)):\n",
        "  temp = {}\n",
        "  temp[\"label\"] = 0\n",
        "  temp[\"text\"] = neg_reviews[i]\n",
        "  temp = pd.DataFrame(temp, index=[0])\n",
        "  if i < int(len(neg_reviews) * test_percentage):\n",
        "    df_test = pd.concat([df_test, temp], ignore_index = True)\n",
        "    df_test.reset_index()\n",
        "  else:\n",
        "    df_train = pd.concat([df_train, temp], ignore_index = True)\n",
        "    df_train.reset_index()\n",
        "\n",
        "for i in range(len(pos_reviews)):\n",
        "  temp = {}\n",
        "  temp[\"label\"] = 1\n",
        "  temp[\"text\"] = pos_reviews[i]\n",
        "  temp = pd.DataFrame(temp, index=[0])\n",
        "  if i < int(len(pos_reviews) * test_percentage):\n",
        "    df_test = pd.concat([df_test, temp], ignore_index = True)\n",
        "    df_test.reset_index()\n",
        "  else:\n",
        "    df_train = pd.concat([df_train, temp], ignore_index = True)\n",
        "    df_train.reset_index()\n",
        "\n",
        "\n",
        "dataset_train = datasets.Dataset.from_pandas(df_train)\n",
        "dataset_test = datasets.Dataset.from_pandas(df_test)\n",
        "\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    return james_h_tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=512)\n",
        "\n",
        "tokenized_train = dataset_train.map(tokenize_function, batched=True)\n",
        "tokenized_test = dataset_test.map(tokenize_function, batched=True)\n",
        "\n",
        "metric = evaluate.load(\"accuracy\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    return metric.compute(predictions=predictions, references=labels)"
      ],
      "metadata": {
        "id": "rpWxbWgcVFNR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### See the [documentation](https://huggingface.co/transformers/v3.0.2/main_classes/trainer.html#transformers.TrainingArguments) what different hyperparameters can be adjusted for the fine-tuning.\n",
        "\n",
        "### Play around with different parameters, specifically with:\n",
        "- learning rate\n",
        "- weight decay\n",
        "- max gradient norm\n",
        "- adam epsilon\n",
        "- num_train_epochs\n",
        "\n",
        "\n",
        "What happens when you change them? Does the fine-tuning take shorter? Longer? Does the performance improve or worsen? Try a few different configurations, record the results, and hypothesize (or if you can, explain!) the reason why theses hyperparameter changes had the observed effect.  \n",
        "\n",
        "*Note: Do NOT change the **output_dir** or **use_cpu** or **evalulation_strategy** variables.*  \n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "QhwUIynSmXWK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#----------------------------------------------\n",
        "# Your code starts here\n",
        "#   Please add comments or text cells in between to explain the general idea of each block of the code.\n",
        "#   Please feel free to add more cells below this cell if necessary\n",
        "#   See https://huggingface.co/transformers/v3.0.2/main_classes/trainer.html#transformers.TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(output_dir=\"finetuned_movie_sentiment\", use_cpu=False, evaluation_strategy=\"epoch\", )\n",
        "\n",
        "#----------------------------------------------\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=james_h_model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train,\n",
        "    eval_dataset=tokenized_test,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "SYC3wL5dl9Hz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the code cell below, evaluate the performance of your chosen pre-trained model on the v2.0 polarity dataset. What's the accuracy, F1 Score, and other metrics."
      ],
      "metadata": {
        "id": "f3bfNPEFMxKd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "james_h_finetuned_model = AutoModelForSequenceClassification.from_pretrained(\"./finetuned_movie_sentiment/checkpoint-500\")\n",
        "\n",
        "#----------------------------------------------\n",
        "# Your code starts here\n",
        "#   Please add comments or text cells in between to explain the general idea of each block of the code.\n",
        "#   Please feel free to add more cells below this cell if necessary\n"
      ],
      "metadata": {
        "id": "22FUzyxfLJFC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_G65MbRfQx5y"
      },
      "source": [
        "## Problem 5 (10 points): Accuracy is not everything!  How fast are the algorithms versus their accuracy?\n",
        "**Compare the runtime of your  baseline algorithms to the runtime of the pre-trained Hugging Face model and the fine-tuned Hugging Face model**\n",
        "\n",
        "**The jupyter command %timeit can be used to measure how long a calculation takes https://ipython.readthedocs.io/en/stable/interactive/magics.html.**\n",
        "* How long does it take to run the \"predict\" function on the entirety of v2.0 polarity dataset on the Sci-Kit Learn models and the Hugging Face models? Can you explain why? Make a table showing your results.\n",
        "* Which method has the best ability in predicting the sentiment correctly? Can you explain why?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Smz1f8tyQtfX"
      },
      "outputs": [],
      "source": [
        "#----------------------------------------------\n",
        "# Your code starts here\n",
        "#   Please add comments or text cells in between to explain the general idea of each block of the code.\n",
        "#   Please feel free to add more cells below this cell if necessary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "o0bcrrUzQtfX"
      },
      "source": [
        "\n",
        "## Problem 6 (20 points): Business question"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j8VZdOxfQtfX"
      },
      "source": [
        "* Suppose you had a machine learning algorithm that could detect the sentiment of NewsAPI articles that was highly accurate.  What kind of business could you build around that?\n",
        "* Who would be your competitors, and what are their sizes?\n",
        "* What would be the size of the market for your product?\n",
        "* In addition, assume that your machine learning was slow to train, but fast in making predictions on new data.  How would that affect your business plan?\n",
        "* How could you use the cloud to support your product?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d-g3psMyQtfX"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EhHQoH_KQtfX"
      },
      "source": [
        "# Slides (for a 5-8 minute presentation) (20 points)\n",
        "\n",
        "\n",
        "1. (5 points) Motivation about the data collection, why the topic is interesting to you.\n",
        "\n",
        "2. (10 points) Communicating Results (figure/table)\n",
        "\n",
        "3. (5 points) Story telling (How all the parts (data, analysis, result) fit together as a story?)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kl5sy8EUQtfY"
      },
      "source": [
        "\n",
        "# Done\n",
        "\n",
        "All set!\n",
        "\n",
        "** What do you need to submit?**\n",
        "\n",
        "* **Notebook File**: Save this IPython notebook, and find the notebook file in your folder (for example, \"filename.ipynb\"). This is the file you need to submit. Please make sure all the plotted tables and figures are in the notebook. If you used \"ipython notebook --pylab=inline\" to open the notebook, all the figures and tables should have shown up in the notebook.\n",
        "\n",
        "\n",
        "* **PPT Slides**: please prepare PPT slides (for 10 minutes' talk) to present about the case study . We will ask two teams which are randomly selected to present their case studies in class for this case study.\n",
        "\n",
        "* **Report**: please prepare a report (less than 10 pages) to report what you found in the data.\n",
        "    * What data you collected?\n",
        "    * Why this topic is interesting or important to you? (Motivations)\n",
        "    * How did you analyse the data?\n",
        "    * What did you find in the data?\n",
        "\n",
        "     (please include figures or tables in the report, but no source code)\n",
        "\n",
        "\n",
        "*Please compress all the files into a single zipped file.*\n",
        "\n",
        "\n",
        "** How to submit: **\n",
        "\n",
        "        Please submit through canvas.wpi.edu\n",
        "\n",
        "### DS3010 Case Study 3 Team ??\n",
        "\n",
        "#### where ?? is your team number.\n",
        "        \n",
        "** Note: Each team just needs to submits one submission **"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "HylAvN1gQtfY"
      },
      "source": [
        "# Grading Criteria:\n",
        "\n",
        "**Total Points: 100**\n",
        "\n",
        "\n",
        "---------------------------------------------------------------------------\n",
        "**Notebook results:**\n",
        "    Points: 80\n",
        "\n",
        "\n",
        "    -----------------------------------\n",
        "    Question 1:\n",
        "    Points: 10\n",
        "    -----------------------------------\n",
        "    \n",
        "    -----------------------------------\n",
        "    Question 2:\n",
        "    Points: 10\n",
        "    -----------------------------------\n",
        "        \n",
        "    -----------------------------------\n",
        "    Question 3:\n",
        "    Points: 15\n",
        "    -----------------------------------\n",
        "  \n",
        "    -----------------------------------\n",
        "    Question 4:  \n",
        "    Points: 15\n",
        "    -----------------------------------\n",
        "\n",
        "    -----------------------------------\n",
        "    Question 5:  \n",
        "    Points: 10\n",
        "    -----------------------------------\n",
        "\n",
        "    -----------------------------------\n",
        "    Question 6:  \n",
        "    Points: 20\n",
        "    -----------------------------------\n",
        "\n",
        "---------------------------------------------------------------------------\n",
        "**Slides (for a 5-8 minute presentation): Story-telling**\n",
        "    Points: 20\n",
        "\n",
        "\n",
        "1. Motivation about the data collection, why the topic is interesting to you.\n",
        "    Points: 5\n",
        "\n",
        "2. Communicating Results (figure/table)\n",
        "    Points: 10\n",
        "\n",
        "3. Story telling (How all the parts (data, analysis, result) fit together as a story?)\n",
        "    Points: 5\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ch582GFeQtfY"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernel_info": {
      "name": "python3"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "nteract": {
      "version": "0.2.0"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}